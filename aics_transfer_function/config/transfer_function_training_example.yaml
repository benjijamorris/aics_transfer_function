# basic parameters
name      : 20to100         # job name: give any string to to name the expriement
seed      : 42              # seed for reproducibility 
verbose   : False           # 'if specified, print debug information before running
debug     : False           # whether to use debug mode

datapath:
  source  : /allen/aics/assay-dev/users/Jianxu/data/TF_demo_test/source/
  target  : /allen/aics/assay-dev/users/Jianxu/data/TF_demo_test/target/

normalization:
  source:
    method: simple_norm
    params: ("middle_otsu", 1.0, 15.5)
  target:
    method: simple_norm
    params: ("middle_otsu", 1.0, 19.5)

load_trained_model:
  path        : /allen/aics/assay-dev/computational/data/transfer_function_feasibility/data_for_paper/FBL_20x_100x/training_result_paper/0627_2246_10b8_20to100_URAz_955983/checkpoints/20to100 
  epoch       : latest # which epoch to load? set to latest to use latest cached model
  load_iter   : -1             # which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]

network:
  model       : pix2pix        # chooses which model to use, only "pix2pix" for now.
  netD        : n_layers       # 'specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')
  netG        : unet_256       # 'specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')
  input_nc    : 1              # 'number of input image channels: 3 for RGB and 1 for grayscale')
  output_nc   : 1              # 'number of output image channels: 3 for RGB and 1 for grayscale')
  ngf         : 96             # number of gen filters in the last conv layer')
  ndf         : 64             # number of discrim filters in the first conv layer')
  n_layers_D  : 3              # 'only used if netD==n_layers'
  no_dropout  : False          # 'no dropout for the generator'
  norm        : batch          # 'instance normalization or batch normalization [instance | batch]'
  gan_mode    : vanilla        # the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')
  lambda_L1   : 1000.0
  init_type   : normal         # 'network initialization [normal | xavier | kaiming | orthogonal]'
  init_gain   : 0.02           # 'scaling factor for normal, xavier and orthogonal.'
  input_patch_size  : [32,256,256]  # patch size (z_depth,y_height,x_width)
  batch_size        : 1       # number of images training simultaneously in one iteration, 1 is preferred as default

training_setting:
  imgs_per_epoch    : -1      # how many images to use? -1 means all
  patches_per_epoch : 50      # a positive integer or -1 (all overlapping patches), 1500 
  niter             : 100     # number of iter using the starting learning rate
  niter_decay       : 500     # number of iter to linearly decay learning rate to zero
  beta1             : 0.5     # momentum term of adam')
  lr                : 0.00002 # initial learning rate for adam')
  lr_policy         : linear  # learning rate policy: linear | step | plateau | cosine
  lr_decay_iters    : 50      # multiply by a gamma every lr_decay_iters iterations')


save:
  results_folder    : /allen/aics/assay-dev/users/Jianxu/data/TF_demo_test/    #models are saved here
  save_training_inspections: True # Save real A,B and fake images for every print_freq
  save_latest_freq  : 5000 # frequency of saving the latest results')
  save_epoch_freq   : 5 # frequency of saving checkpoints at the end of epochs')
  print_freq        : 5000  # frequency of showing training results on console
